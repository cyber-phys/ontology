{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62768c78",
   "metadata": {},
   "source": [
    "BERTopic Demo\n",
    "https://maartengr.github.io/BERTopic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57491f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bertopic\n",
    "%pip install bertopic[flair, gensim, spacy, use]\n",
    "%pip install openai\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc57d82b",
   "metadata": {},
   "source": [
    "First we will create a sqlite database for the chatgpt conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a60a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import chatgpt_db_manager as cm\n",
    "\n",
    "# Parse JSON and create the database\n",
    "db_file = 'chat.db' # Database File\n",
    "chatgpt_export = 'conversations.json' #ChatGPT export file\n",
    "cm.create_database(db_file)\n",
    "conversation_infos, all_chats = cm.parse_json(chatgpt_export)\n",
    "\n",
    "# Insert conversations and chats with updated UUIDs\n",
    "cm.insert_conversations_and_chats(db_file, conversation_infos, all_chats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083465b4",
   "metadata": {},
   "source": [
    "Now we will calculate topics for chat messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa085df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "conn = cm.connect_db(db_file)\n",
    "\n",
    "# Fetch messages and uuids\n",
    "chats = cm.fetch_all_chats(conn)\n",
    "conn.close()\n",
    "messages = chats[\"message\"]\n",
    "ids = chats[\"id\"]\n",
    "\n",
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(messages)\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(topic_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38183068",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = topic_model.get_document_info(messages)['Name'].tolist()\n",
    "conn = cm.connect_db(db_file)\n",
    "cm.insert_chat_topics(conn,ids,topic_names)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40faa6f6",
   "metadata": {},
   "source": [
    "Now we will calculate topics for chat pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42a0c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = cm.connect_db(db_file)\n",
    "\n",
    "chats = cm.fetch_message_pairs(conn)\n",
    "conn.close()\n",
    "messages = chats[\"message\"]\n",
    "parent_ids = chats[\"parent_id\"]\n",
    "child_ids = chats[\"child_id\"]\n",
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(messages)\n",
    "topic_names = topic_model.get_document_info(messages)['Name'].tolist()\n",
    "conn = cm.connect_db(db_file)\n",
    "cm.insert_chat_links(conn, parent_ids, child_ids,topic_names)\n",
    "conn.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7806fea",
   "metadata": {},
   "source": [
    "Now we will calculate Topics for Conversations. \n",
    "\n",
    "# TODO we need to fix fetch_conversations_with_chats it is reurning a list of lists for [\"message\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cb4a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = cm.connect_db(db_file)\n",
    "\n",
    "# Fetch messages and uuids\n",
    "conversations = cm.fetch_conversations_with_chats(conn)\n",
    "print(conversations[2][\"message\"][0])\n",
    "for chats in conversations:\n",
    "    messages = chats[\"message\"]\n",
    "    id = chats[\"conversation_id\"][0]\n",
    "    topic_model = BERTopic()\n",
    "    topics, probs = topic_model.fit_transform(messages)\n",
    "    # topic_names = topic_model.get_document_info(messages)['Name'].tolist()\n",
    "    # cm.insert_chat_topics(conn,id,topic_names)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95f38bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7f8eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "topic_model_cluster = BERTopic(hdbscan_model=hdbscan_model)\n",
    "\n",
    "from embedding import fetch_entries\n",
    "db_name = \"law_database_old.db\"\n",
    "texts, uuids = fetch_entries(db_name)\n",
    "topics_cluster, probs_cluster = topic_model_cluster.fit_transform(texts)\n",
    "\n",
    "topic_model_cluster.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03fd150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedding import fetch_entries\n",
    "db_name = \"law_database_old.db\"\n",
    "docs, uuids = fetch_entries(db_name)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "\n",
    "# Prepare embeddings\n",
    "# docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = sentence_model.encode(docs, show_progress_bar=False)\n",
    "\n",
    "# Train BERTopic\n",
    "topic_model = BERTopic().fit(docs, embeddings)\n",
    "\n",
    "# Run the visualization with the original embeddings\n",
    "topic_model.visualize_documents(docs, embeddings=embeddings)\n",
    "\n",
    "# Reduce dimensionality of embeddings, this step is optional but much faster to perform iteratively:\n",
    "reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "# topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings)\n",
    "print(reduced_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2a2f16",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b21969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# print(len(topic_model.get_document_info(docs)))\n",
    "# print(len(reduced_embeddings))\n",
    "\n",
    "# document_info = topic_model.get_document_info(docs)\n",
    "# topics = document_info['Name']\n",
    "# documents = document_info['Document']\n",
    "\n",
    "# # Assuming `reduced_embeddings` is your UMAP data and it's a list of lists or a NumPy array\n",
    "# data = [{\"umap_x\": float(point[0]), \"umap_y\": float(point[1]), \"topic\": topic, \"document\": document} for point, topic, document in zip(reduced_embeddings, topics, documents)]\n",
    "\n",
    "# # Save the data as a JSON file\n",
    "# with open(\"umap-data.json\", \"w\") as f:\n",
    "#     json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7718bcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # Get the document information from BERTopic\n",
    "# d = topic_model.get_document_info(texts)\n",
    "# d_doc = d['Document']\n",
    "# d_name = d['Name']\n",
    "\n",
    "# # Convert to list if necessary\n",
    "# s_doc = d_doc.tolist() if hasattr(d_doc, 'tolist') else d_doc\n",
    "# s_name = d_name.tolist() if hasattr(d_name, 'tolist') else d_name\n",
    "\n",
    "# # Create a list of dictionaries for JSON output\n",
    "# data = []\n",
    "# for i, doc in enumerate(s_doc):\n",
    "#     # Use the UUID corresponding to the document's text\n",
    "#     law_entry_uuid = uuids[i]\n",
    "#     data.append({\n",
    "#         \"umap_x\": float(reduced_embeddings[i][0]),\n",
    "#         \"umap_y\": float(reduced_embeddings[i][1]),\n",
    "#         \"item_id\": law_entry_uuid  # Add the law_entry_uuid here\n",
    "#     })\n",
    "\n",
    "# # Save the data as a JSON file\n",
    "# with open(\"umap-data.json\", \"w\") as f:\n",
    "#     json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d54633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy as sch\n",
    "from bertopic import BERTopic\n",
    "# topic_model = BERTopic()\n",
    "# topics, probs = topic_model.fit_transform(messages)\n",
    "\n",
    "# Hierarchical topics\n",
    "linkage_function = lambda x: sch.linkage(x, 'single', optimal_ordering=True)\n",
    "hierarchical_topics = topic_model.hierarchical_topics(messages, linkage_function=linkage_function)\n",
    "cm.insert_hierarchical_topics_as_dag(conn, hierarchical_topics)\n",
    "tree = topic_model.get_topic_tree(hierarchical_topics)\n",
    "# print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd1725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "# Extract mapping from ID to name\n",
    "topic_to_name = dict(zip(hier_topics.Child_Left_ID, hier_topics.Child_Left_Name))\n",
    "topic_to_name.update(dict(zip(hier_topics.Child_Right_ID, hier_topics.Child_Right_Name)))\n",
    "topic_to_name = {topic: name[:100] for topic, name in topic_to_name.items()}\n",
    "\n",
    "# Create tree\n",
    "tree = {str(row[1].Parent_ID): [str(row[1].Child_Left_ID), str(row[1].Child_Right_ID)]\n",
    "        for row in hier_topics.iterrows()}\n",
    "\n",
    "\n",
    "def create_hierarchical_topic_dag(hier_topics: pd.DataFrame) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Create a Directed Acyclic Graph (DAG) from hierarchical topics.\n",
    "\n",
    "    Arguments:\n",
    "        hier_topics: A DataFrame containing the hierarchical topic structure.\n",
    "                     This is the output of `topic_model.hierarchical_topics()`.\n",
    "\n",
    "    Returns:\n",
    "        A networkx DiGraph representing the hierarchical topic connections.\n",
    "    \"\"\"\n",
    "    # Initialize directed graph\n",
    "    dag = nx.DiGraph()\n",
    "\n",
    "    # Add nodes with topic names as node attributes\n",
    "    for _, row in hier_topics.iterrows():\n",
    "        dag.add_node(row['Parent_ID'], name=row['Parent_Name'])\n",
    "        dag.add_node(row['Child_Left_ID'], name=row['Child_Left_Name'])\n",
    "        dag.add_node(row['Child_Right_ID'], name=row['Child_Right_Name'])\n",
    "\n",
    "        # Add edges from parent to children\n",
    "        dag.add_edge(row['Parent_ID'], row['Child_Left_ID'])\n",
    "        dag.add_edge(row['Parent_ID'], row['Child_Right_ID'])\n",
    "\n",
    "    return dag\n",
    "\n",
    "# Example usage\n",
    "# Assuming `hierarchical_topics` is already defined as in your notebook\n",
    "dag = create_hierarchical_topic_dag(hierarchical_topics)\n",
    "\n",
    "# Optionally, visualize the DAG\n",
    "# Note: You need to have matplotlib installed (`pip install matplotlib`)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw(dag, with_labels=True, node_size=20, node_color=\"lightblue\", font_size=10, font_weight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8ae6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build rdf\n",
    "import chatgpt_db_manager as cm\n",
    "db_file = \"chat.db\"\n",
    "conn = cm.connect_db(db_file)\n",
    "\n",
    "chat_links = cm.fetch_chat_links(conn)\n",
    "rdf_triples = []\n",
    "for link in chat_links:\n",
    "    triple = f\"{link['source_chat_id']} {link['label']} {link['target_chat_id']}\"\n",
    "    next_message_triple = f\"{link['source_chat_id']} {link['source_author']}_to_{link['target_author']} {link['target_chat_id']}\"\n",
    "    rdf_triples.append(triple)\n",
    "    rdf_triples.append(next_message_triple)\n",
    "\n",
    "chat_links = cm.fetch_chat_topics(conn)\n",
    "for link in chat_links:\n",
    "    triple = f\"{link['label']} chat_type {link['chat_id']}\"\n",
    "    rdf_triples.append(triple)\n",
    "\n",
    "# Example output of rdf_triples\n",
    "for triple in rdf_triples[:5]:  # Print the first 5 triples for demonstration\n",
    "    print(triple)\n",
    "\n",
    "topic_links = cm.fetch_topic_links(conn)\n",
    "print(topic_links)\n",
    "for link in topic_links:\n",
    "    triple = f\"{link['parent_label']} is_member_of {link['child_label']}\"\n",
    "    rdf_triples.append(triple)\n",
    "\n",
    "# Example output of rdf_triples\n",
    "for triple in rdf_triples[:5]:  # Print the first 5 triples for demonstration\n",
    "    print(triple)\n",
    "\n",
    "with open(\"valid.txt\", \"w\") as file:\n",
    "    for triple in rdf_triples:\n",
    "        file.write(triple + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1b3bd6",
   "metadata": {},
   "source": [
    "Future Work:\n",
    "Dynamic Topic Modeling over time\n",
    "https://maartengr.github.io/BERTopic/getting_started/topicsovertime/topicsovertime.html\n",
    "\n",
    "Topics per class:\n",
    "https://maartengr.github.io/BERTopic/getting_started/topicsperclass/topicsperclass.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
